\documentclass[12pt, a4paper, oneside]{article}
\usepackage{CJKutf8}
\usepackage{amsmath, amsthm, amssymb, bm, color, framed, graphicx, hyperref, mathrsfs}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage[left=20mm, right=20mm, top=20mm, bottom=20mm]{geometry}
\pagestyle{fancy}
\fancyhf{}
\rhead{\begin{CJK}{UTF8}{gbsn}{计83 李天勤 2018080106}\end{CJK}}
\lhead{\begin{CJK}{UTF8}{gbsn}{高代线性代数作业2}\end{CJK}}
	

\title{\textbf{课程作业}}
\author{AkaCoder404}
\date{\today}
\newpage
\linespread{1.5}
\definecolor{shadecolor}{RGB}{241, 241, 255}
\newcounter{problemname}
\newenvironment{problem}{\begin{shaded}\stepcounter{problemname}\par\noindent\textbf{题目\arabic{problemname}. }}{\end{shaded}\par}
\newenvironment{solution}{\par\noindent\textbf{解答. }}{\par}
\newenvironment{note}{\par\noindent\textbf{题目\arabic{problemname}的注记. }}{\par}

\begin{document}
\begin{CJK}{UTF8}{gbsn}

% \maketitle
% \newpage

\section{1.2 Invariant Decomposition}

\setcounter{problemname}{0}
\begin{problem}
  Prove or find counter examples \newline
  1. For four subspaces, if any three of them are linearly independent, then the four subspaces are linearly independent \newline
  2. If subspaces $V_1,V_2$ are linearly independent, and $V_1,V_3,V_4$ are linearly independent, and $V_2,V_3,V_4$ are linearly independent, then all four subspaces are linearly independent \newline
  3. If $V_1,V_2$ are linearly independent, and $V_3, V_4$ are linearly independent, and $V_1+V_2,V_3+V_4$ are linearly independent, then all four subspaces are linearly independent 
\end{problem}

\begin{solution} \newline
  1. Let the subspaces be $W,X,Y,Z$ are subspace of $S$ where $W$ has set of vectors $\vec{w_1}, \dots,\vec{w_n}$ which follows the rules of subspaces (closed under addition and closed under scalar multiplication, and so on for $X,Y,Z$ \newline 
  Two subspaces are linear independent if and only if $W \cap X = \{0\}$. Which means that for a subspace to be linearly independent, it means no linear combination of one element from each of $W, X$ is equal to $0$ except for the trivial case.  In other words, they are linearly independent if and only if $\text{span}(W) \cup \text{span}(X) = {0}$ \newline 
  Thus, if three of the subspaces are linearly independent, then the union’s of there span’s is an empty intersection. However, just because three of the subspaces are linearly independent, it does not mean the fourth is. The span of the fourth, let’s say $Z$, may intersect with the any of the previous four.
  We can use the following as a counter example \newline
  $$ V_1 = \text{span}\left\{ \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix} \right\},  V_2 = \text{span}\left\{ \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} \right\},  V_3 = \text{span}\left\{ \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} \right\},  V_4 = \text{span}\left\{ \begin{pmatrix} 0 \\ 1 \\ 1 \end{pmatrix} \right\}$$
  2. We can use the same example as 1. \newline
  3. If 
  $$ W \text{basis} : W_1, \dots W_p $$
  $$ X \text{basis} : X_1, \dots X_q $$
  $$ Y \text{basis} : Y_1, \dots Y_r $$
  $$ Z \text{basis} : Z_1, \dots Z_s $$
  And we know that $W$,$X$ are linearly independent, and so are $Y,Z$, then 
  $$ W + X \text{basis} : W_1,\dots,W_p, X_1,\dots,X_q $$ 
  $$ Y + Z \text{basis} : Y_1,\dots,Y_r, Z_1,\dots,Z_s $$
  and if $W + X$, $Y + Z$ are linearly independent, then
  $$ \alpha_1 W_1+ \cdots + \alpha_{p}W_p + \beta_1 X_1 + \cdots \beta_q X_q + \lambda_1 Y_1 + \cdots  + \lambda_r Y_r + \zeta_1Z_1 + \cdots + \zeta_sZ_s  = 0$$
  which means all the coefficients are equal to zero. Which states that four vectors are linearly independent.
\end{solution}


\begin{problem}
  Let V be the space of $n\times n$ real matrices. Let $T: V\rightarrow V$ be the transpose operations. Find the non-trivial $T$-invariant decomposition of $V$ and find the corresponding block form of $T$
\end{problem}

\begin{solution} 
  Let us define $e_{ij}$ to be a matrix with a value of 1 at entry $(i,j)$ and zero elsewhere. This is the basis for the space $M_{n\times n}$.

  Thus the transpose operation on that space is as follows: 
  
  $$ T(e_{ij})=e_{ji} $$
  
  Thus, we need to come up with arbitrary ordering of $\{e_{ij}\}$. Lets say that we have the matrix in $\mathbb{R}^2$ 
  $$ A = \begin{pmatrix}
    a & b \\ c & d
  \end{pmatrix}$$
  A possible b  asis for the vector space is as such. 
  $$ S = \left\{ \begin{pmatrix} 1 & 0 \\ 0 & 0  \end{pmatrix}, \begin{pmatrix} 0 & 1 \\ 0 & 0  \end{pmatrix}, \begin{pmatrix} 0 & 0 \\ 1 & 0  \end{pmatrix} , \begin{pmatrix} 0 & 0 \\ 0 & 1  \end{pmatrix} \right\},$$
  Where the standard can be represented as $S=\{E_{11},E_{12},E_{21},E_{22}\}$ , so the Matrix A can also be written as $aE_{11} + bE_{12} + cE_{21} + dE_{22}$, which can be represented as vector
  $$ v  = \begin{pmatrix}
    a \\ b \\ c \\ d
  \end{pmatrix}$$ 
  Then the linear mapping $T$ acts on the elements of the basis, giving us the assoicated matrix of 
  $$ \begin{pmatrix}
    1 & 0 & 0 & 0 \\
    0 & 0 & 1 & 0 \\ 
    0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 1
  \end{pmatrix}$$
  And we can get a block form from this matrix.
\end{solution}


\begin{problem}
  Let $p(x)$ be any polynomial, and define $p(A)$ in the obvious manner. Prove \newline
  1. If $AB=BA$, show that $\text{Ker}(B), \text{Ran}(B)$ are both $A$-invariant subspaces. \newline
  2. Prove that $Ap(A)=p(A)A$ \newline
  3. Conclude that $N_{\infty}(A-\lambda I), R_{\infty}(A-\lambda I)$ are both $A$ invariant for any $\lambda\in \mathbb{C}$
\end{problem}

\begin{solution} \newline
  $(1.1)$ $x \in \text{Ker}(B), Bx = 0, ABx=B(Ax)=0$, therefore, $Ax\in \text{Ker}(B)$ \newline
  $(1.2)$ $x \in \text{Ran}(B), \exists y, x =By, Ax = ABy=B(Ay)$, therefore, $Ax\in \text{Ran}(B)$ \newline
  $(2)$ $A\cdot A^k = A^k \cdot A, AI = IA$, thefore, $A\cdot P(A), P(A) \cdot A$ \newline
  $(3.1)$ $x \in N_{\infty}(A-\lambda I), \therefore \exists (A-\lambda I)^x = 0$ \newline
  $\therefore (A-\lambda I)^k(Ak) = A((A-\lambda I)^kx) = 0 $ \newline
  $ Ax \in N_{\infty(A-\lambda I)} $ \newline
  $(3.2)$ $x\in R_{\infty} (A -\lambda I), \therefore \forall k, \exists y_k, (A -\lambda I)^ky_k = x$ \newline
  $\therefore Ax = A(A-\lambda I)^y_k = (A -\lambda I)^k(Ay_k)$ \newline
  And if we let $y_k' = Ay_k, \forall k, \exists y_k', (A-\lambda I)^ky_k' = Ax$ 
\end{solution}



\begin{problem}
  Note that any linear map must have at least one eigenvector. Fix any two $n\times n$ matrices, $A,B$. Suppose $AB=BA$ \newline
  1. If $W$ is an $A$ invariant subspace, show that $A$ has an eigenvector in $W$ \newline
  2. Show that $\text{Ker}(A- \lambda I)$ is always B-invariant for all $\lambda\in \mathbb{C}$ \newline
  3. Show that $A,B$ has a common eigenvector. 
\end{problem}

\begin{solution} \newline
  $(1)$ Because $W$ is an $A$ invarient space, and if we  $V_1 = (V_{1} \cdots V_{t})$ be the basis of $W$ and if 
  $$ A = X \begin{pmatrix}
    A_1 & & \\
    & \ddots & \\
    & & A_t  
  \end{pmatrix} X^{-1}, X = (v_1, \cdots v_t) 
  $$ 
  $v_2, \cdots ,v_t$ are other $A$ invarient subspaces, Therefore $A_1$ must have eigenvectorm and thus $\vec{x}$ is an eigenvector of A in W. \newline
  $(2)$ $x \in \text{Ker}(A - \lambda I)$  \newline
  $$ \Rightarrow (A-\lambda I)x = 0$$
  $$ \Rightarrow (A-\lambda I)Bx = B(A - \lambda I)x = 0$$
  $$ \Rightarrow Bx \in \text{Ker} (A - \lambda I) $$
  $(3)$ Because $\text{Ker}(A-\lambda I)$ is a $B$-invariant subspace \newline
  $\Rightarrow$ B has an eigen vector in $\text{Ker}(A - \lambda I)$ \newline
  $\Rightarrow Bx = \lambda_o x, x \in \text{Ker}(A-\lambda I)$ \newline
  $\Rightarrow (A - \lambda I) x = 0 \Rightarrow Ax = \lambda x$ \newline
  $x$ is the common eigenvector of both $A$ and $B$ 
\end{solution}

\vfill
\vspace{0.25\textheight}
\newpage





\end{CJK}
\end{document}